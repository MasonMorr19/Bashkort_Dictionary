"""
Content Scraper for Bashkir Reading Practice
============================================
Web scraping and content curation for reading practice materials.

Features:
- Curated Bashkir text sources
- Content difficulty grading
- Vocabulary extraction and highlighting
- Offline content caching
"""

import json
import hashlib
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass, field
from enum import Enum

# Optional imports
try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False

try:
    from bs4 import BeautifulSoup
    BS4_AVAILABLE = True
except ImportError:
    BS4_AVAILABLE = False


class DifficultyLevel(Enum):
    """Reading difficulty levels."""
    BEGINNER = 1
    ELEMENTARY = 2
    INTERMEDIATE = 3
    ADVANCED = 4
    NATIVE = 5


@dataclass
class ReadingText:
    """A reading practice text."""
    id: str
    title: str
    content: str
    source: str
    difficulty: DifficultyLevel
    word_count: int
    vocabulary: List[str] = field(default_factory=list)
    topics: List[str] = field(default_factory=list)
    audio_url: Optional[str] = None
    created_at: str = field(default_factory=lambda: datetime.now().isoformat())

    def to_dict(self) -> Dict:
        return {
            'id': self.id,
            'title': self.title,
            'content': self.content,
            'source': self.source,
            'difficulty': self.difficulty.name,
            'word_count': self.word_count,
            'vocabulary': self.vocabulary,
            'topics': self.topics,
            'audio_url': self.audio_url,
            'created_at': self.created_at
        }

    @classmethod
    def from_dict(cls, data: Dict) -> 'ReadingText':
        return cls(
            id=data['id'],
            title=data['title'],
            content=data['content'],
            source=data['source'],
            difficulty=DifficultyLevel[data['difficulty']],
            word_count=data['word_count'],
            vocabulary=data.get('vocabulary', []),
            topics=data.get('topics', []),
            audio_url=data.get('audio_url'),
            created_at=data.get('created_at', datetime.now().isoformat())
        )


class ContentScraper:
    """
    Service for scraping and curating Bashkir reading content.

    Provides graded reading materials from various sources,
    with vocabulary extraction and difficulty assessment.
    """

    # Curated content sources (for reference, actual scraping requires permission)
    CONTENT_SOURCES = {
        'wikipedia': {
            'name': 'Bashkir Wikipedia',
            'url': 'https://ba.wikipedia.org',
            'type': 'encyclopedia',
            'difficulty': DifficultyLevel.INTERMEDIATE
        },
        'bashinform': {
            'name': 'Bashinform News',
            'url': 'https://www.bashinform.ru/news/',
            'type': 'news',
            'difficulty': DifficultyLevel.ADVANCED
        }
    }

    # Built-in reading texts (curated, no scraping needed)
    BUILTIN_TEXTS = [
        {
            'id': 'intro_001',
            'title': 'ÒºÐ°ÑƒÐ¼Ñ‹Ò»Ñ‹Ò“Ñ‹Ò™! (Hello!)',
            'content': '''ÒºÐ°ÑƒÐ¼Ñ‹Ò»Ñ‹Ò“Ñ‹Ò™! ÐœÐ¸Ð½ â€” Ó˜Ð¼Ð¸Ð½Ó™. ÐœÐ¸Ð½ Ó¨Ñ„Ó©Ð»Ó™ Ð¹Ó™ÑˆÓ™Ð¹ÐµÐ¼.
Ó¨Ñ„Ó© â€” Ð‘Ð°ÑˆÒ¡Ð¾Ñ€Ñ‚Ð¾ÑÑ‚Ð°Ð½ Ð±Ð°ÑˆÒ¡Ð°Ð»Ð°Ò»Ñ‹. Ð£Ð» Ð¼Ð°Ñ‚ÑƒÑ€ Ò¡Ð°Ð»Ð°.
ÐœÐ¸Ð½ÐµÒ£ Ò“Ð°Ð¸Ð»Ó™Ð¼ Ò™ÑƒÑ€: Ð°Ñ‚Ð°Ð¹, Ó™ÑÓ™Ð¹, Ð¾Ð»Ð°Ñ‚Ð°Ð¹, Ó©Ð»Ó™ÑÓ™Ð¹, Ò¡ÑƒÑÑ‚Ñ‹Ð¼ Ò»Ó™Ð¼ Ò»ÐµÒ£Ð»ÐµÐ¼.
ÐœÐ¸Ð½ Ð±Ð°ÑˆÒ¡Ð¾Ñ€Ñ‚ Ñ‚ÐµÐ»ÐµÐ½ Ò»Ó©Ð¹Ó™Ð¼. Ð£Ð» Ð¼Ð°Ñ‚ÑƒÑ€ Ñ‚ÐµÐ»!''',
            'source': 'Curated beginner text',
            'difficulty': DifficultyLevel.BEGINNER,
            'topics': ['greetings', 'family', 'city'],
            'vocabulary': ['Ò»Ð°ÑƒÐ¼Ñ‹Ò»Ñ‹Ò“Ñ‹Ò™', 'Ð¼Ð¸Ð½', 'Ð¹Ó™ÑˆÓ™Ð¹ÐµÐ¼', 'Ð±Ð°ÑˆÒ¡Ð°Ð»Ð°', 'Ð¼Ð°Ñ‚ÑƒÑ€', 'Ò“Ð°Ð¸Ð»Ó™', 'Ð°Ñ‚Ð°Ð¹', 'Ó™ÑÓ™Ð¹', 'Ñ‚ÐµÐ»']
        },
        {
            'id': 'intro_002',
            'title': 'ÐœÐ¸Ð½ÐµÒ£ ÐºÓ©Ð½Ó©Ð¼ (My Day)',
            'content': '''ÐœÐ¸Ð½ Ð¸Ñ€Ñ‚Ó™Ð½ Ò»Ó™Ò“Ó™Ñ‚ ÐµÑ‚ÐµÐ»Ó™ Ñ‚Ð¾Ñ€Ð°Ð¼. Ó¨Ð¹Ò™Ó™ Ð¸Ñ€Ñ‚Ó™Ð½Ð³Ðµ Ð°Ñˆ Ð°ÑˆÐ°Ð¹Ñ‹Ð¼.
ÒºÑƒÒ£Ñ‹Ð½Ð°Ð½ Ð¼Ó™ÐºÑ‚Ó™Ð¿ÐºÓ™ Ð±Ð°Ñ€Ð°Ð¼. ÐœÓ™ÐºÑ‚Ó™Ð¿Ñ‚Ó™ Ð´Ó™Ñ€ÐµÑÑ‚Ó™Ñ€ Ð±Ð°Ñ€.
ÐšÓ©Ð½ ÑƒÑ€Ñ‚Ð°Ò»Ñ‹Ð½Ð´Ð° ÑÐ» Ð¸Ñ‚Ó™Ð¼ Ò»Ó™Ð¼ Ñ‚Ó©ÑˆÐºÓ© Ð°Ñˆ Ð°ÑˆÐ°Ð¹Ñ‹Ð¼.
ÐšÐ¸ÑÑ‚Ó™Ð½ Ó©Ð¹Ð³Ó™ Ò¡Ð°Ð¹Ñ‚Ð°Ð¼. ÐšÐ¸ÑÐºÓ™ Ó©Ð¹ ÑÑˆÑ‚Ó™Ñ€ÐµÐ½ ÑÑˆÐ»Ó™Ð¹ÐµÐ¼.
Ð¢Ó©Ð½Ð»Ó™ Ð¹Ð¾Ò¡Ð»Ð°Ð¹Ñ‹Ð¼. Ð¯Ò¡ÑˆÑ‹ Ñ‚Ó©Ð½Ð´Ó™Ñ€!''',
            'source': 'Curated beginner text',
            'difficulty': DifficultyLevel.BEGINNER,
            'topics': ['daily routine', 'time', 'school'],
            'vocabulary': ['Ð¸Ñ€Ñ‚Ó™Ð½', 'Ñ‚Ð¾Ñ€Ð°Ð¼', 'Ð°Ñˆ', 'Ð¼Ó™ÐºÑ‚Ó™Ð¿', 'Ð´Ó™Ñ€ÐµÑ', 'ÑÐ»', 'Ó©Ð¹', 'Ñ‚Ó©Ð½', 'Ð¹Ð¾Ò¡Ð»Ð°Ð¹Ñ‹Ð¼']
        },
        {
            'id': 'nature_001',
            'title': 'Ð‘Ð°ÑˆÒ¡Ð¾Ñ€Ñ‚Ð¾ÑÑ‚Ð°Ð½ Ñ‚Ó™Ð±Ð¸Ò“Ó™Ñ‚Ðµ (Nature of Bashkortostan)',
            'content': '''Ð‘Ð°ÑˆÒ¡Ð¾Ñ€Ñ‚Ð¾ÑÑ‚Ð°Ð½ â€” Ð¼Ð°Ñ‚ÑƒÑ€ Ð¸Ð». Ð£Ð½Ð´Ð° Ñ‚Ð°ÑƒÒ™Ð°Ñ€, ÑƒÑ€Ð¼Ð°Ð½Ð´Ð°Ñ€, Ð¹Ñ‹Ð»Ò“Ð°Ð»Ð°Ñ€ Ð±Ð°Ñ€.
Ð£Ñ€Ð°Ð» Ñ‚Ð°ÑƒÒ™Ð°Ñ€Ñ‹ â€” Ð¸Ò£ Ð±ÐµÐ¹ÐµÐº Ñ‚Ð°ÑƒÒ™Ð°Ñ€. Ð¯Ð¼Ð°Ð½Ñ‚Ð°Ñƒ â€” Ð¸Ò£ Ð±ÐµÐ¹ÐµÐº Ñ‚Ð°Ñƒ.
ÐÒ“Ð¸Ò™ÐµÐ» â€” Ð¸Ò£ Ð¾Ò™Ð¾Ð½ Ð¹Ñ‹Ð»Ò“Ð°. Ð£Ð» Ð£Ñ€Ð°Ð»Ð»Ð°Ð½ Ð°Ò“Ñ‹Ð¿ ÑÑ‹Ò“Ð°.
Ð£Ñ€Ð¼Ð°Ð½Ð´Ð°Ñ€Ò™Ð° Ð°Ð¹Ñ‹ÑƒÒ™Ð°Ñ€, Ð±Ò¯Ñ€ÐµÐ»Ó™Ñ€, Ñ‚Ó©Ð»ÐºÓ©Ð»Ó™Ñ€ Ð¹Ó™ÑˆÓ™Ð¹.
Ð‘Ð°ÑˆÒ¡Ð¾Ñ€Ñ‚Ð¾ÑÑ‚Ð°Ð½ â€” ÑƒÐ¼Ð°Ñ€Ñ‚Ð° Ð¸Ð»Ðµ. Ð‘Ð°ÑˆÒ¡Ð¾Ñ€Ñ‚ Ð±Ð°Ð»Ñ‹ â€” Ð±Ð¸Ðº Ñ‚Ó™Ð¼Ð»Ðµ!''',
            'source': 'Curated elementary text',
            'difficulty': DifficultyLevel.ELEMENTARY,
            'topics': ['nature', 'geography', 'animals'],
            'vocabulary': ['Ñ‚Ó™Ð±Ð¸Ò“Ó™Ñ‚', 'Ñ‚Ð°Ñƒ', 'ÑƒÑ€Ð¼Ð°Ð½', 'Ð¹Ñ‹Ð»Ò“Ð°', 'Ð±ÐµÐ¹ÐµÐº', 'Ð°Ð¹Ñ‹Ñƒ', 'Ð±Ò¯Ñ€Ðµ', 'Ð±Ð°Ð»', 'Ñ‚Ó™Ð¼Ð»Ðµ']
        },
        {
            'id': 'legend_001',
            'title': 'Ð£Ñ€Ð°Ð» Ð±Ð°Ñ‚Ñ‹Ñ€ Ñ‚ÑƒÑ€Ð°Ò»Ñ‹Ð½Ð´Ð° (About Ural-Batyr)',
            'content': '''Ð£Ñ€Ð°Ð» Ð±Ð°Ñ‚Ñ‹Ñ€ â€” Ð±Ð°ÑˆÒ¡Ð¾Ñ€Ñ‚ Ñ…Ð°Ð»Ò¡Ñ‹Ð½Ñ‹Ò£ Ð»ÐµÐ³ÐµÐ½Ð´Ð°Ñ€ Ð³ÐµÑ€Ð¾Ð¹Ñ‹.
Ð£Ð» Ð™Ó™Ð½Ð±Ð¸Ñ€Ò™Ó™ Ò»Ó™Ð¼ Ð™Ó™Ð½Ð±Ð¸ÐºÓ™ Ò“Ð°Ð¸Ð»Ó™Ò»ÐµÐ½Ð´Ó™ Ñ‚Ñ‹ÑƒÒ“Ð°Ð½.
Ð£Ñ€Ð°Ð» Ð±Ð°Ñ‚Ñ‹Ñ€Ò™Ñ‹Ò£ Ð°Ò“Ð°Ò»Ñ‹ â€” Ð¨Ò¯Ð»Ð³Ó™Ð½.
Ð£Ñ€Ð°Ð» Ð±Ð°Ñ‚Ñ‹Ñ€ ÑÑƒÑ‹Ð·Ð»Ñ‹Ò¡ Ð¼ÐµÐ½Ó™Ð½ ÐºÓ©Ñ€Ó™ÑˆÑ‚Ðµ. Ð£Ð» Ò®Ð»ÐµÐ¼Ð´Ðµ ÐµÒ£Ð´Ðµ.
Ò®Ð»Ð³Ó™Ñ, Ð£Ñ€Ð°Ð» Ð±Ð°Ñ‚Ñ‹Ñ€ Ñ‚Ð°Ñƒ Ð±ÑƒÐ»Ñ‹Ð¿ Ò¡Ð°Ð»Ð´Ñ‹. Ð£Ð» Ñ‚Ð°Ñƒ Ñ…Ó™Ò™ÐµÑ€ Ð£Ñ€Ð°Ð» Ñ‚Ð°ÑƒÒ™Ð°Ñ€Ñ‹ Ñ‚Ð¸Ð¿ Ð°Ñ‚Ð°Ð»Ð°.
Ð¨Ò¯Ð»Ð³Ó™Ð½ Ð¼Ó™Ò£Ð³ÐµÐ»ÐµÐº Ñ…Ó™Ñ‚ÐµÑ€Ð³Ó™ Ò¡Ð°Ð»Ð´Ñ‹ Ð¨Ò¯Ð»Ð³Ó™Ð½Ñ‚Ð°Ñˆ Ð¼Ó™Ð¼ÐµÑ€Ð¹Ó™Ò»ÐµÐ½Ð´Ó™.''',
            'source': 'Curated intermediate text',
            'difficulty': DifficultyLevel.INTERMEDIATE,
            'topics': ['mythology', 'Ural-Batyr', 'legend'],
            'vocabulary': ['Ð±Ð°Ñ‚Ñ‹Ñ€', 'Ñ…Ð°Ð»Ñ‹Ò¡', 'Ð»ÐµÐ³ÐµÐ½Ð´Ð°', 'Ñ‚Ñ‹ÑƒÒ“Ð°Ð½', 'ÐºÓ©Ñ€Ó™Ñˆ', 'ÐµÒ£ÐµÒ¯', 'Ñ‚Ð°Ñƒ', 'Ð¼Ó™Ð¼ÐµÑ€Ð¹Ó™']
        },
        {
            'id': 'history_001',
            'title': 'Ð‘Ð°ÑˆÒ¡Ð¾Ñ€Ñ‚Ð¾ÑÑ‚Ð°Ð½ Ñ‚Ð°Ñ€Ð¸Ñ…Ñ‹ (History of Bashkortostan)',
            'content': '''Ð‘Ð°ÑˆÒ¡Ð¾Ñ€Ñ‚Ñ‚Ð°Ñ€ â€” Ð£Ñ€Ð°Ð» Ñ‚Ð°ÑƒÒ™Ð°Ñ€Ñ‹Ð½Ð´Ð° Ð¹Ó™ÑˆÓ™Ð³Ó™Ð½ Ð¸Ò«ÐºÐµ Ñ…Ð°Ð»Ñ‹Ò¡.
XVI Ð±Ñ‹ÑƒÐ°Ñ‚Ñ‚Ð° Ð±Ð°ÑˆÒ¡Ð¾Ñ€Ñ‚Ñ‚Ð°Ñ€ Ð Ó™ÑÓ™Ð¹ ÑÐ¾ÑÑ‚Ð°Ð²Ñ‹Ð½Ð° Ð¸Ð½Ð´Ðµ.
1917 Ð¹Ñ‹Ð»Ð´Ð° Ð‘Ð°ÑˆÒ¡Ð¾Ñ€Ñ‚Ð¾ÑÑ‚Ð°Ð½ Ð°Ð²Ñ‚Ð¾Ð½Ð¾Ð¼Ð¸ÑÒ»Ñ‹ Ð±ÑƒÐ»Ð´Ñ‹.
1919 Ð¹Ñ‹Ð»Ð´Ð° Ð‘Ð°ÑˆÒ¡Ð¾Ñ€Ñ‚ ÐÐ¡Ð¡Ð  Ð¾Ð¹Ð¾ÑˆÑ‚Ð¾Ñ€Ð¾Ð»Ð´Ð¾ â€” Ñ‚Ó™Ò¯Ð³Ðµ Ð°Ð²Ñ‚Ð¾Ð½Ð¾Ð¼ Ñ€ÐµÑÐ¿ÑƒÐ±Ð»Ð¸ÐºÐ°.
1990 Ð¹Ñ‹Ð»Ð´Ð° Ð‘Ð°ÑˆÒ¡Ð¾Ñ€Ñ‚Ð¾ÑÑ‚Ð°Ð½ ÑÑƒÐ²ÐµÑ€ÐµÐ½Ð¸Ñ‚ÐµÑ‚ ÑÑ€Ð°Ð»Ñ‹ÑˆÑ‚Ñ‹.
Ð‘Ó©Ð³Ó©Ð½ Ð‘Ð°ÑˆÒ¡Ð¾Ñ€Ñ‚Ð¾ÑÑ‚Ð°Ð½ â€” Ð Ó™ÑÓ™Ð¹ Ð¤ÐµÐ´ÐµÑ€Ð°Ñ†Ð¸ÑÒ»Ñ‹Ð½Ñ‹Ò£ Ñ€ÐµÑÐ¿ÑƒÐ±Ð»Ð¸ÐºÐ°Ò»Ñ‹.
4 Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½ ÐºÐµÑˆÐµ Ð¹Ó™ÑˆÓ™Ð¹. Ð‘Ð°ÑˆÒ¡Ð°Ð»Ð° â€” Ó¨Ñ„Ó©.''',
            'source': 'Curated intermediate text',
            'difficulty': DifficultyLevel.INTERMEDIATE,
            'topics': ['history', 'politics', 'autonomy'],
            'vocabulary': ['Ñ‚Ð°Ñ€Ð¸Ñ…', 'Ñ…Ð°Ð»Ñ‹Ò¡', 'Ð±Ñ‹ÑƒÐ°Ñ‚', 'Ð°Ð²Ñ‚Ð¾Ð½Ð¾Ð¼Ð¸Ñ', 'Ñ€ÐµÑÐ¿ÑƒÐ±Ð»Ð¸ÐºÐ°', 'ÑÑƒÐ²ÐµÑ€ÐµÐ½Ð¸Ñ‚ÐµÑ‚', 'Ñ„ÐµÐ´ÐµÑ€Ð°Ñ†Ð¸Ñ']
        },
        {
            'id': 'culture_001',
            'title': 'Ð¡Ð°Ð±Ð°Ð½Ñ‚ÑƒÐ¹ (Sabantuy Festival)',
            'content': '''Ð¡Ð°Ð±Ð°Ð½Ñ‚ÑƒÐ¹ â€” Ð±Ð°ÑˆÒ¡Ð¾Ñ€Ñ‚ Ò»Ó™Ð¼ Ñ‚Ð°Ñ‚Ð°Ñ€ Ñ…Ð°Ð»Ñ‹Ò¡Ñ‚Ð°Ñ€Ñ‹Ð½Ñ‹Ò£ Ð¸Ò£ Ò™ÑƒÑ€ Ð±Ó™Ð¹Ñ€Ó™Ð¼Ðµ.
"Ð¡Ð°Ð±Ð°Ð½" â€” "Ò»Ð°Ð±Ð°Ð½" (Ð¿Ð»ÑƒÐ³), "Ñ‚ÑƒÐ¹" â€” "Ð±Ó™Ð¹Ñ€Ó™Ð¼" Ñ‚Ð¸Ð³Ó™Ð½Ð´Ðµ Ð°Ò£Ð»Ð°Ñ‚Ð°.
Ð¡Ð°Ð±Ð°Ð½Ñ‚ÑƒÐ¹ ÑÒ™Ò™Ð°, ÑÓ™ÑÐµÒ¯ ÑÑˆÑ‚Ó™Ñ€Ðµ Ñ‚Ð°Ð¼Ð°Ð¼Ð»Ð°Ð½Ò“Ð°Ñ, Ò¯Ñ‚ÐºÓ™Ñ€ÐµÐ»Ó™.

Ð‘Ó™Ð¹Ñ€Ó™Ð¼Ð´Ó™ ÐºÒ¯Ð¿ ÑƒÐ¹Ñ‹Ð½Ð´Ð°Ñ€ Ð±Ð°Ñ€:
â€” ÐšÓ©Ñ€Ó™Ñˆ â€” Ð¼Ð¸Ð»Ð»Ð¸ ÐºÓ©Ñ€Ó™Ñˆ, Ð¸Ò£ ÐºÓ©ÑÐ»Ó© Ð±Ð°Ñ‚Ñ‹Ñ€ Ð±ÑƒÐ»Ñ‹Ñ€Ò“Ð°
â€” ÐÑ‚ ÑÐ°Ð±Ñ‹ÑˆÑ‹ â€” Ð°Ñ‚Ð»Ð°Ñ€ Ð¹Ò¯Ð³ÐµÑ€ÐµÑˆÐµ
â€” Ò Ð°Ð¿-Ò¡Ð°Ð¿ â€” Ò¡Ð°Ð¿Ð»Ñ‹ Ð¹Ò¯Ð³ÐµÑ€ÐµÑˆ
â€” Ò Ð°ÑˆÑ‹Ò¡ Ð¼ÐµÐ½Ó™Ð½ Ð¹Ð¾Ð¼Ð¾Ñ€Ñ‚Ò¡Ð° â€” Ñ‚Ð¸Ò™ Ð¹Ò¯Ð³ÐµÑ€ÐµÒ¯

Ð¡Ð°Ð±Ð°Ð½Ñ‚ÑƒÐ¹Ò™Ð° Ð¹Ñ‹Ñ€Ð»Ð°Ð¹Ò™Ð°Ñ€, Ð±ÐµÐ¹ÐµÐ¹Ò™Ó™Ñ€, Ò¡ÑƒÑ€Ð°Ð¹ ÑƒÐ¹Ð½Ð°Ð¹Ò™Ð°Ñ€.
Ð‘Ó©Ñ‚Ó™ Ñ…Ð°Ð»Ñ‹Ò¡ Ð±ÐµÑ€Ð³Ó™ ÑˆÐ°Ñ‚Ð»Ð°Ð½Ð°!''',
            'source': 'Curated intermediate text',
            'difficulty': DifficultyLevel.INTERMEDIATE,
            'topics': ['culture', 'festival', 'traditions'],
            'vocabulary': ['Ð±Ó™Ð¹Ñ€Ó™Ð¼', 'ÐºÓ©Ñ€Ó™Ñˆ', 'Ð°Ñ‚ ÑÐ°Ð±Ñ‹ÑˆÑ‹', 'ÑƒÐ¹Ñ‹Ð½', 'Ð¹Ñ‹Ñ€', 'Ð±ÐµÐ¹ÐµÒ¯', 'Ò¡ÑƒÑ€Ð°Ð¹', 'Ñ…Ð°Ð»Ñ‹Ò¡']
        },
        {
            'id': 'literature_001',
            'title': 'ÐœÐ¾ÑÑ‚Ð°Ð¹ ÐšÓ™Ñ€Ð¸Ð¼ â€” Ð±Ð°ÑˆÒ¡Ð¾Ñ€Ñ‚ ÑˆÐ°Ò“Ð¸Ñ€Ñ‹ (Mustai Karim â€” Bashkir Poet)',
            'content': '''ÐœÐ¾ÑÑ‚Ð°Ð¹ ÐšÓ™Ñ€Ð¸Ð¼ (1919-2005) â€” Ð±Ó©Ð¹Ó©Ðº Ð±Ð°ÑˆÒ¡Ð¾Ñ€Ñ‚ ÑÒ™Ñ‹ÑƒÑÑ‹Ò»Ñ‹ Ò»Ó™Ð¼ ÑˆÐ°Ò“Ð¸Ñ€Ñ‹.
Ð£Ð» ÐšÐ»ÑÑˆÐµÐ²Ð¾ Ð°ÑƒÑ‹Ð»Ñ‹Ð½Ð´Ð° Ñ‚Ñ‹ÑƒÒ“Ð°Ð½. Ó˜Ò«Ó™Ð» Ð¸ÑÐµÐ¼Ðµ â€” ÐœÐ¾ÑÑ‚Ð°Ñ„Ð° Ð¡Ð°Ñ„Ð¸Ñ‡ ÐšÐ°Ñ€Ð¸Ð¼Ð¾Ð².

ÐœÐ¾ÑÑ‚Ð°Ð¹ ÐšÓ™Ñ€Ð¸Ð¼ ÐºÒ¯Ð¿ Ó™Ò«Ó™Ñ€Ò™Ó™Ñ€ ÑÒ™Ò“Ð°Ð½:
â€” "ÐžÒ™Ð¾Ð½-Ð¾Ò™Ð°Ò¡ Ð±Ð°Ð»Ð° ÑÐ°Ò¡" â€” Ð°Ð²Ñ‚Ð¾Ð±Ð¸Ð¾Ð³Ñ€Ð°Ñ„Ð¸Ðº Ð¿Ð¾Ð²ÐµÑÑ‚ÑŒ
â€” "ÐÐ¹ Ñ‚Ð¾Ñ‚Ð¾Ð»Ò“Ð°Ð½ Ñ‚Ó©Ð½Ð´Ó™" â€” Ð´Ñ€Ð°Ð¼Ð°
â€” "Ò®Ð»Ð¼Ó™Ò«Ð±Ð°Ð¹" â€” Ð¿ÑŒÐµÑÐ°
â€” ÐšÒ¯Ð¿ ÑˆÐ¸Ò“Ñ‹Ñ€Ò™Ð°Ñ€ Ò»Ó™Ð¼ Ð¿Ð¾ÑÐ¼Ð°Ð»Ð°Ñ€

ÐœÐ¾ÑÑ‚Ð°Ð¹ ÐšÓ™Ñ€Ð¸Ð¼ Ð¡Ð¾Ñ†Ð¸Ð°Ð»Ð¸ÑÑ‚Ð¸Ðº Ð¥ÐµÒ™Ð¼Ó™Ñ‚ Ð“ÐµÑ€Ð¾Ð¹Ñ‹ Ð¸ÑÐµÐ¼ÐµÐ½ Ð°Ð»Ð´Ñ‹.
Ð£Ð½Ñ‹Ò£ Ó™Ò«Ó™Ñ€Ò™Ó™Ñ€Ðµ ÐºÒ¯Ð¿ Ñ‚ÐµÐ»Ð³Ó™ Ñ‚Ó™Ñ€Ð¶ÐµÐ¼Ó™ Ð¸Ñ‚ÐµÐ»Ð´Ðµ.
Ó¨Ñ„Ó©Ð»Ó™ ÐœÐ¾ÑÑ‚Ð°Ð¹ ÐšÓ™Ñ€Ð¸Ð¼ Ð¸ÑÐµÐ¼ÐµÐ½Ð´Ó™Ð³Ðµ Ñ‚ÐµÐ°Ñ‚Ñ€ Ò»Ó™Ð¼ Ð¼ÑƒÐ·ÐµÐ¹ Ð±Ð°Ñ€.''',
            'source': 'Curated advanced text',
            'difficulty': DifficultyLevel.ADVANCED,
            'topics': ['literature', 'poetry', 'famous people'],
            'vocabulary': ['ÑˆÐ°Ò“Ð¸Ñ€', 'ÑÒ™Ñ‹ÑƒÑÑ‹', 'Ó™Ò«Ó™Ñ€', 'Ð¿Ð¾Ð²ÐµÑÑ‚ÑŒ', 'Ð´Ñ€Ð°Ð¼Ð°', 'ÑˆÐ¸Ò“Ñ‹Ñ€', 'Ð¿Ð¾ÑÐ¼Ð°', 'Ñ‚Ó™Ñ€Ð¶ÐµÐ¼Ó™']
        },
        {
            'id': 'proverbs_001',
            'title': 'Ð‘Ð°ÑˆÒ¡Ð¾Ñ€Ñ‚ Ð¼Ó™Ò¡Ó™Ð»Ð´Ó™Ñ€Ðµ (Bashkir Proverbs)',
            'content': '''Ð‘Ð°ÑˆÒ¡Ð¾Ñ€Ñ‚ Ñ…Ð°Ð»Ò¡Ñ‹Ð½Ñ‹Ò£ ÐºÒ¯Ð¿ Ð°Ò¡Ñ‹Ð»Ð»Ñ‹ Ð¼Ó™Ò¡Ó™Ð»Ð´Ó™Ñ€Ðµ Ð±Ð°Ñ€:

ðŸ´ ÐÑ‚ â€” Ð¸Ñ€ Ò¡Ð°Ð½Ð°Ñ‚Ñ‹.
(A horse is a man's wings.)

ðŸ  ÐÑ‚Ð° Ð¹Ð¾Ñ€Ñ‚Ð¾ â€” Ð°Ð»Ñ‚Ñ‹Ð½ Ð±Ð¸ÑˆÐµÐº.
(Father's home is a golden cradle.)

ðŸ“š Ð‘ÐµÐ»ÐµÐ¼ â€” Ð±Ð°Ð¹Ð»Ñ‹Ò¡, Ð±ÐµÐ»ÐµÐ¼Ò»ÐµÒ™Ð»ÐµÐº â€” ÑÑ€Ð»Ñ‹Ð»Ñ‹Ò¡.
(Knowledge is wealth, ignorance is poverty.)

ðŸ¤ Ð‘ÐµÑ€Ò™Ó™Ð¼Ð»ÐµÐº Ð±ÑƒÐ»Ò»Ð° â€” Ñ‚ÐµÑ€ÐµÐºÐ»ÐµÐº Ð±ÑƒÐ»Ñ‹Ñ€.
(Where there is unity, there is life.)

ðŸ’ª Ð­Ñˆ Ð±ÐµÑ‚ÐºÓ™Ñ ÑƒÐ¹Ð½Ð°.
(Play after work is done.)

ðŸŒ¾ Ð¥ÐµÒ™Ð¼Ó™Ñ‚ Ð¸Ñ‚Ò»Ó™Ò£, Ñ€Ð¸Ð·Ñ‹Ò¡ Ñ‚Ð°Ð±Ñ‹Ñ€Ò»Ñ‹Ò£.
(If you work, you will find sustenance.)

ðŸ—£ï¸ Ð¢ÐµÐ» â€” Ð±Ð°Ð», Ñ‚ÐµÐ» â€” Ð±Ð°Ð» Ò¡Ð¾Ñ€Ñ‚Ð¾.
(The tongue is honey, the tongue is a bee.)

â¤ï¸ Ð¢ÑƒÒ“Ð°Ð½ ÐµÑ€ â€” Ð°Ð»Ñ‚Ñ‹Ð½ Ð±Ð¸ÑˆÐµÐº.
(Native land is a golden cradle.)''',
            'source': 'Curated elementary text',
            'difficulty': DifficultyLevel.ELEMENTARY,
            'topics': ['proverbs', 'wisdom', 'culture'],
            'vocabulary': ['Ð¼Ó™Ò¡Ó™Ð»', 'Ð°Ñ‚', 'Ð¹Ð¾Ñ€Ñ‚', 'Ð±ÐµÐ»ÐµÐ¼', 'Ñ…ÐµÒ™Ð¼Ó™Ñ‚', 'Ñ‚ÐµÐ»', 'Ñ‚ÑƒÒ“Ð°Ð½ ÐµÑ€']
        }
    ]

    def __init__(self, cache_dir: Optional[str] = None):
        """
        Initialize content scraper.

        Args:
            cache_dir: Directory for caching content
        """
        self.cache_dir = Path(cache_dir) if cache_dir else Path('data/reading_cache')
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self._texts: Dict[str, ReadingText] = {}
        self._load_builtin_texts()

    def _load_builtin_texts(self):
        """Load built-in curated texts."""
        for text_data in self.BUILTIN_TEXTS:
            text = ReadingText(
                id=text_data['id'],
                title=text_data['title'],
                content=text_data['content'],
                source=text_data['source'],
                difficulty=text_data['difficulty'],
                word_count=len(text_data['content'].split()),
                vocabulary=text_data.get('vocabulary', []),
                topics=text_data.get('topics', [])
            )
            self._texts[text.id] = text

    @property
    def is_scraping_available(self) -> bool:
        """Check if web scraping dependencies are available."""
        return REQUESTS_AVAILABLE and BS4_AVAILABLE

    def get_all_texts(self) -> List[ReadingText]:
        """Get all available reading texts."""
        return list(self._texts.values())

    def get_text_by_id(self, text_id: str) -> Optional[ReadingText]:
        """Get a specific text by ID."""
        return self._texts.get(text_id)

    def get_texts_by_difficulty(self, difficulty: DifficultyLevel) -> List[ReadingText]:
        """Get texts filtered by difficulty level."""
        return [t for t in self._texts.values() if t.difficulty == difficulty]

    def get_texts_by_topic(self, topic: str) -> List[ReadingText]:
        """Get texts filtered by topic."""
        topic_lower = topic.lower()
        return [t for t in self._texts.values()
                if any(topic_lower in t.lower() for t in t.topics)]

    def analyze_difficulty(self, text: str, dictionary: List[Dict]) -> DifficultyLevel:
        """
        Analyze text difficulty based on vocabulary coverage.

        Args:
            text: Text to analyze
            dictionary: Word dictionary for known words

        Returns:
            Estimated difficulty level
        """
        # Extract words
        words = set(re.findall(r'\b[Ð°-ÑÓ™Ó©Ò¯Ò“Ò¡Ò£Ò»Ò™Ñ‘Ð-Ð¯Ó˜Ó¨Ò®Ò’Ò Ò¢ÒºÒ˜Ð]+\b', text.lower()))

        # Get dictionary words
        dict_words = {w['bashkir'].lower() for w in dictionary}

        # Calculate coverage
        if not words:
            return DifficultyLevel.BEGINNER

        known_words = words & dict_words
        coverage = len(known_words) / len(words)

        # Average word length
        avg_length = sum(len(w) for w in words) / len(words)

        # Sentence complexity (words per sentence)
        sentences = re.split(r'[.!?]', text)
        sentences = [s for s in sentences if s.strip()]
        words_per_sentence = len(words) / max(len(sentences), 1)

        # Determine difficulty
        if coverage > 0.8 and avg_length < 5 and words_per_sentence < 8:
            return DifficultyLevel.BEGINNER
        elif coverage > 0.6 and avg_length < 6 and words_per_sentence < 12:
            return DifficultyLevel.ELEMENTARY
        elif coverage > 0.4 and words_per_sentence < 15:
            return DifficultyLevel.INTERMEDIATE
        elif coverage > 0.2:
            return DifficultyLevel.ADVANCED
        else:
            return DifficultyLevel.NATIVE

    def extract_vocabulary(self, text: str, dictionary: List[Dict]) -> List[Dict]:
        """
        Extract vocabulary from text with dictionary matches.

        Args:
            text: Text to analyze
            dictionary: Word dictionary

        Returns:
            List of vocabulary items with translations
        """
        # Extract unique words
        words = set(re.findall(r'\b[Ð°-ÑÓ™Ó©Ò¯Ò“Ò¡Ò£Ò»Ò™Ñ‘Ð-Ð¯Ó˜Ó¨Ò®Ò’Ò Ò¢ÒºÒ˜Ð]+\b', text.lower()))

        # Build lookup
        dict_lookup = {w['bashkir'].lower(): w for w in dictionary}

        vocabulary = []
        for word in sorted(words):
            if word in dict_lookup:
                entry = dict_lookup[word]
                vocabulary.append({
                    'word': entry['bashkir'],
                    'english': entry.get('english', ''),
                    'russian': entry.get('russian', ''),
                    'in_dictionary': True
                })
            else:
                vocabulary.append({
                    'word': word,
                    'english': '',
                    'russian': '',
                    'in_dictionary': False
                })

        return vocabulary

    def highlight_vocabulary(
        self,
        text: str,
        known_words: set,
        dictionary: List[Dict]
    ) -> str:
        """
        Add HTML highlighting to vocabulary in text.

        Args:
            text: Original text
            known_words: Set of words the user has learned
            dictionary: Word dictionary

        Returns:
            HTML-formatted text with highlighting
        """
        dict_words = {w['bashkir'].lower() for w in dictionary}

        def replace_word(match):
            word = match.group(0)
            word_lower = word.lower()

            if word_lower in known_words:
                # Known word - green
                return f'<span class="known-word" style="color: #00AF66;">{word}</span>'
            elif word_lower in dict_words:
                # In dictionary but not learned - blue (clickable)
                return f'<span class="dict-word" style="color: #0066B3; cursor: pointer;" data-word="{word_lower}">{word}</span>'
            else:
                # Unknown word - gray
                return f'<span class="unknown-word" style="color: #888;">{word}</span>'

        # Replace Cyrillic words
        highlighted = re.sub(
            r'\b[Ð°-ÑÓ™Ó©Ò¯Ò“Ò¡Ò£Ò»Ò™Ñ‘Ð-Ð¯Ó˜Ó¨Ò®Ò’Ò Ò¢ÒºÒ˜Ð]+\b',
            replace_word,
            text
        )

        return highlighted

    def add_custom_text(
        self,
        title: str,
        content: str,
        source: str,
        difficulty: DifficultyLevel,
        topics: List[str] = None
    ) -> ReadingText:
        """
        Add a custom reading text.

        Args:
            title: Text title
            content: Text content
            source: Source attribution
            difficulty: Difficulty level
            topics: List of topics

        Returns:
            Created ReadingText object
        """
        # Generate ID
        text_id = f"custom_{hashlib.md5(content.encode()).hexdigest()[:8]}"

        text = ReadingText(
            id=text_id,
            title=title,
            content=content,
            source=source,
            difficulty=difficulty,
            word_count=len(content.split()),
            topics=topics or [],
            vocabulary=[]
        )

        self._texts[text_id] = text
        self._save_to_cache(text)

        return text

    def _save_to_cache(self, text: ReadingText):
        """Save a text to the cache."""
        cache_file = self.cache_dir / f"{text.id}.json"
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(text.to_dict(), f, ensure_ascii=False, indent=2)

    def load_from_cache(self):
        """Load cached texts."""
        for cache_file in self.cache_dir.glob('*.json'):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    text = ReadingText.from_dict(data)
                    self._texts[text.id] = text
            except Exception as e:
                print(f"Error loading cached text {cache_file}: {e}")

    def get_reading_stats(self, completed_texts: List[str]) -> Dict:
        """
        Get reading statistics.

        Args:
            completed_texts: List of completed text IDs

        Returns:
            Statistics dictionary
        """
        total_texts = len(self._texts)
        completed_count = len(completed_texts)

        # Words read
        words_read = sum(
            self._texts[tid].word_count
            for tid in completed_texts
            if tid in self._texts
        )

        # By difficulty
        by_difficulty = {}
        for level in DifficultyLevel:
            level_texts = self.get_texts_by_difficulty(level)
            completed_at_level = [t for t in level_texts if t.id in completed_texts]
            by_difficulty[level.name] = {
                'total': len(level_texts),
                'completed': len(completed_at_level)
            }

        return {
            'total_texts': total_texts,
            'completed_texts': completed_count,
            'completion_rate': completed_count / total_texts if total_texts else 0,
            'words_read': words_read,
            'by_difficulty': by_difficulty
        }


# Singleton instance
_content_scraper: Optional[ContentScraper] = None


def get_content_scraper() -> ContentScraper:
    """Get or create the content scraper singleton."""
    global _content_scraper
    if _content_scraper is None:
        _content_scraper = ContentScraper()
    return _content_scraper
